{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmtUxsiB3JglDTRJxk55yB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vicky-0222/NLP/blob/master/lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Обработка естественного языка**"
      ],
      "metadata": {
        "id": "uJMvjYTYXMMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Устанавлеваем необходимые библиотеки"
      ],
      "metadata": {
        "id": "mwop5clXXgtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hsGN89bW91V",
        "outputId": "e3f3e85d-72f5-4882-df1b-b028ddb9d9a2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Импорт библиотек и данных"
      ],
      "metadata": {
        "id": "3yacoUXPX9kt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mitgIl9QQ9c0"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pymorphy3\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_ru = 'Универсального рецепта того, как выбрать правильный, единственно верный, только тебе предназначенный путь в жизни, просто нет и быть не может.'\n",
        "text_en = 'There is no universal recipe for how to choose the right, the only true, only you destined path in life.'"
      ],
      "metadata": {
        "id": "_WrgPhkWebyb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Провести на любом тексте лемматизацию и стемминг (nltk, pymorphy2, pymorphy3, natasha)\n"
      ],
      "metadata": {
        "id": "E-5g74ROYC6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "morph = pymorphy3.MorphAnalyzer()\n",
        "stemmer_ru = SnowballStemmer(\"russian\")\n",
        "stemmer_en = SnowballStemmer(\"english\")\n",
        "\n",
        "def lemmatize(text: str) -> list[str]:\n",
        "    words = word_tokenize(text)\n",
        "    lemmatized_words = [morph.parse(word)[0].normal_form for word in words]\n",
        "    return lemmatized_words\n",
        "\n",
        "# стемминг для кирилицы\n",
        "def stem_ru(text: str) -> list[str]:\n",
        "    words = word_tokenize(text)\n",
        "    stemmed_words = [stemmer_ru.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "# стемминг для латиницы\n",
        "def stem_en(text: str) -> list[str]:\n",
        "    words = word_tokenize(text)\n",
        "    stemmed_words = [stemmer_en.stem(word) for word in words]\n",
        "    return stemmed_words"
      ],
      "metadata": {
        "id": "5LWVB5cUYJeB"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatize(text_ru))\n",
        "print(stem_ru(text_ru))\n",
        "print(lemmatize(text_en))\n",
        "print(stem_en(text_en))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ2ChT__lFyZ",
        "outputId": "96cb586c-0562-4191-fed7-8f73fb777b91"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['универсальный', 'рецепт', 'тот', ',', 'как', 'выбрать', 'правильный', ',', 'единственно', 'верный', ',', 'только', 'ты', 'предназначить', 'путь', 'в', 'жизнь', ',', 'просто', 'нет', 'и', 'быть', 'не', 'мочь', '.']\n",
            "['универсальн', 'рецепт', 'тог', ',', 'как', 'выбра', 'правильн', ',', 'единствен', 'верн', ',', 'тольк', 'теб', 'предназначен', 'пут', 'в', 'жизн', ',', 'прост', 'нет', 'и', 'быт', 'не', 'может', '.']\n",
            "['there', 'is', 'no', 'universal', 'recipe', 'for', 'how', 'to', 'choose', 'the', 'right', ',', 'the', 'only', 'true', ',', 'only', 'you', 'destined', 'path', 'in', 'life', '.']\n",
            "['there', 'is', 'no', 'univers', 'recip', 'for', 'how', 'to', 'choos', 'the', 'right', ',', 'the', 'onli', 'true', ',', 'onli', 'you', 'destin', 'path', 'in', 'life', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Написать функцию для токенизации всех символов из ASCII\n"
      ],
      "metadata": {
        "id": "UVb8yXA7aSZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_ascii(text: str) -> list[str]:\n",
        "    ascii_tokens = [char for char in text if 'a' <= char <= 'z' or 'A' <= char <= 'Z']\n",
        "    return ascii_tokens\n",
        "\n",
        "def tokenize_cyrillic(text: str) -> list[str]:\n",
        "    cyrillic_tokens = [char for char in text if 'а' <= char <= 'я' or 'А' <= char <= 'Я']\n",
        "    return cyrillic_tokens"
      ],
      "metadata": {
        "id": "qf_2ECjkaSAr"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenize_cyrillic(text_ru))\n",
        "print(tokenize_ascii(text_en))"
      ],
      "metadata": {
        "id": "cNU53yNWmD7G",
        "outputId": "db9a01ed-c970-49e2-868b-132fea1146ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['У', 'н', 'и', 'в', 'е', 'р', 'с', 'а', 'л', 'ь', 'н', 'о', 'г', 'о', 'р', 'е', 'ц', 'е', 'п', 'т', 'а', 'т', 'о', 'г', 'о', 'к', 'а', 'к', 'в', 'ы', 'б', 'р', 'а', 'т', 'ь', 'п', 'р', 'а', 'в', 'и', 'л', 'ь', 'н', 'ы', 'й', 'е', 'д', 'и', 'н', 'с', 'т', 'в', 'е', 'н', 'н', 'о', 'в', 'е', 'р', 'н', 'ы', 'й', 'т', 'о', 'л', 'ь', 'к', 'о', 'т', 'е', 'б', 'е', 'п', 'р', 'е', 'д', 'н', 'а', 'з', 'н', 'а', 'ч', 'е', 'н', 'н', 'ы', 'й', 'п', 'у', 'т', 'ь', 'в', 'ж', 'и', 'з', 'н', 'и', 'п', 'р', 'о', 'с', 'т', 'о', 'н', 'е', 'т', 'и', 'б', 'ы', 'т', 'ь', 'н', 'е', 'м', 'о', 'ж', 'е', 'т']\n",
            "['T', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'n', 'o', ' ', 'u', 'n', 'i', 'v', 'e', 'r', 's', 'a', 'l', ' ', 'r', 'e', 'c', 'i', 'p', 'e', ' ', 'f', 'o', 'r', ' ', 'h', 'o', 'w', ' ', 't', 'o', ' ', 'c', 'h', 'o', 'o', 's', 'e', ' ', 't', 'h', 'e', ' ', 'r', 'i', 'g', 'h', 't', ',', ' ', 't', 'h', 'e', ' ', 'o', 'n', 'l', 'y', ' ', 't', 'r', 'u', 'e', ',', ' ', 'o', 'n', 'l', 'y', ' ', 'y', 'o', 'u', ' ', 'd', 'e', 's', 't', 'i', 'n', 'e', 'd', ' ', 'p', 'a', 't', 'h', ' ', 'i', 'n', ' ', 'l', 'i', 'f', 'e', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Написать функцию для векторизации всех символов из ASCII"
      ],
      "metadata": {
        "id": "gS7oqLLNaWz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_ascii(text: list[str]) -> list[int]:\n",
        "    ascii_vector = []\n",
        "    for word in text:\n",
        "        for char in word:\n",
        "            ascii_vector.append(ord(char))\n",
        "    return ascii_vector"
      ],
      "metadata": {
        "id": "r7cc2VGPac4J"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorize_ascii(text_ru))\n",
        "print(vectorize_ascii(text_en))"
      ],
      "metadata": {
        "id": "U9zHkC5mmJPZ",
        "outputId": "de81665d-7425-4187-cba2-6fd6d8775d92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1059, 1085, 1080, 1074, 1077, 1088, 1089, 1072, 1083, 1100, 1085, 1086, 1075, 1086, 32, 1088, 1077, 1094, 1077, 1087, 1090, 1072, 32, 1090, 1086, 1075, 1086, 44, 32, 1082, 1072, 1082, 32, 1074, 1099, 1073, 1088, 1072, 1090, 1100, 32, 1087, 1088, 1072, 1074, 1080, 1083, 1100, 1085, 1099, 1081, 44, 32, 1077, 1076, 1080, 1085, 1089, 1090, 1074, 1077, 1085, 1085, 1086, 32, 1074, 1077, 1088, 1085, 1099, 1081, 44, 32, 1090, 1086, 1083, 1100, 1082, 1086, 32, 1090, 1077, 1073, 1077, 32, 1087, 1088, 1077, 1076, 1085, 1072, 1079, 1085, 1072, 1095, 1077, 1085, 1085, 1099, 1081, 32, 1087, 1091, 1090, 1100, 32, 1074, 32, 1078, 1080, 1079, 1085, 1080, 44, 32, 1087, 1088, 1086, 1089, 1090, 1086, 32, 1085, 1077, 1090, 32, 1080, 32, 1073, 1099, 1090, 1100, 32, 1085, 1077, 32, 1084, 1086, 1078, 1077, 1090, 46]\n",
            "[84, 104, 101, 114, 101, 32, 105, 115, 32, 110, 111, 32, 117, 110, 105, 118, 101, 114, 115, 97, 108, 32, 114, 101, 99, 105, 112, 101, 32, 102, 111, 114, 32, 104, 111, 119, 32, 116, 111, 32, 99, 104, 111, 111, 115, 101, 32, 116, 104, 101, 32, 114, 105, 103, 104, 116, 44, 32, 116, 104, 101, 32, 111, 110, 108, 121, 32, 116, 114, 117, 101, 44, 32, 111, 110, 108, 121, 32, 121, 111, 117, 32, 100, 101, 115, 116, 105, 110, 101, 100, 32, 112, 97, 116, 104, 32, 105, 110, 32, 108, 105, 102, 101, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Провести токенизацию и векторизацию текста после лемматизации и стемминга"
      ],
      "metadata": {
        "id": "gAf5hZbVadOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Для кириллицы"
      ],
      "metadata": {
        "id": "b2Rkdkgv2dVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenize_cyrillic(lemmatize(text_ru))) #токенизация после лемматизации\n",
        "print(vectorize_ascii(lemmatize(text_ru))) #векторизация после лемматизации\n",
        "print(tokenize_cyrillic(stem_ru(text_ru))) #токенизация после стемминга\n",
        "print(vectorize_ascii(stem_ru(text_ru))) #векторизация после стемминга"
      ],
      "metadata": {
        "id": "T4727Y6oaiCG",
        "outputId": "00170cd0-5ea8-4adc-a2ef-a5c74d1447ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['универсальный', 'рецепт', 'тот', 'как', 'выбрать', 'правильный', 'единственно', 'верный', 'только', 'ты', 'предназначить', 'путь', 'в', 'жизнь', 'просто', 'нет', 'и', 'быть', 'не', 'мочь']\n",
            "[1091, 1085, 1080, 1074, 1077, 1088, 1089, 1072, 1083, 1100, 1085, 1099, 1081, 1088, 1077, 1094, 1077, 1087, 1090, 1090, 1086, 1090, 44, 1082, 1072, 1082, 1074, 1099, 1073, 1088, 1072, 1090, 1100, 1087, 1088, 1072, 1074, 1080, 1083, 1100, 1085, 1099, 1081, 44, 1077, 1076, 1080, 1085, 1089, 1090, 1074, 1077, 1085, 1085, 1086, 1074, 1077, 1088, 1085, 1099, 1081, 44, 1090, 1086, 1083, 1100, 1082, 1086, 1090, 1099, 1087, 1088, 1077, 1076, 1085, 1072, 1079, 1085, 1072, 1095, 1080, 1090, 1100, 1087, 1091, 1090, 1100, 1074, 1078, 1080, 1079, 1085, 1100, 44, 1087, 1088, 1086, 1089, 1090, 1086, 1085, 1077, 1090, 1080, 1073, 1099, 1090, 1100, 1085, 1077, 1084, 1086, 1095, 1100, 46]\n",
            "['универсальн', 'рецепт', 'тог', 'как', 'выбра', 'правильн', 'единствен', 'верн', 'тольк', 'теб', 'предназначен', 'пут', 'в', 'жизн', 'прост', 'нет', 'и', 'быт', 'не', 'может']\n",
            "[1091, 1085, 1080, 1074, 1077, 1088, 1089, 1072, 1083, 1100, 1085, 1088, 1077, 1094, 1077, 1087, 1090, 1090, 1086, 1075, 44, 1082, 1072, 1082, 1074, 1099, 1073, 1088, 1072, 1087, 1088, 1072, 1074, 1080, 1083, 1100, 1085, 44, 1077, 1076, 1080, 1085, 1089, 1090, 1074, 1077, 1085, 1074, 1077, 1088, 1085, 44, 1090, 1086, 1083, 1100, 1082, 1090, 1077, 1073, 1087, 1088, 1077, 1076, 1085, 1072, 1079, 1085, 1072, 1095, 1077, 1085, 1087, 1091, 1090, 1074, 1078, 1080, 1079, 1085, 44, 1087, 1088, 1086, 1089, 1090, 1085, 1077, 1090, 1080, 1073, 1099, 1090, 1085, 1077, 1084, 1086, 1078, 1077, 1090, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Для латиницы"
      ],
      "metadata": {
        "id": "A9ZypTdZ2pkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenize_ascii(lemmatize(text_en))) #токенизация после лемматизации\n",
        "print(vectorize_ascii(lemmatize(text_en))) #векторизация после лемматизации\n",
        "print(tokenize_ascii(stem_ru(text_en))) #токенизация после стемминга\n",
        "print(vectorize_ascii(stem_ru(text_en))) #векторизация после стемминга"
      ],
      "metadata": {
        "id": "gE_x-v0u2jyk",
        "outputId": "7322cc98-6db4-45e5-81dd-8051e756ee77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['there', 'is', 'no', 'universal', 'recipe', 'for', 'how', 'to', 'choose', 'the', 'right', 'the', 'only', 'true', 'only', 'you', 'destined', 'path', 'in', 'life']\n",
            "[116, 104, 101, 114, 101, 105, 115, 110, 111, 117, 110, 105, 118, 101, 114, 115, 97, 108, 114, 101, 99, 105, 112, 101, 102, 111, 114, 104, 111, 119, 116, 111, 99, 104, 111, 111, 115, 101, 116, 104, 101, 114, 105, 103, 104, 116, 44, 116, 104, 101, 111, 110, 108, 121, 116, 114, 117, 101, 44, 111, 110, 108, 121, 121, 111, 117, 100, 101, 115, 116, 105, 110, 101, 100, 112, 97, 116, 104, 105, 110, 108, 105, 102, 101, 46]\n",
            "['There', 'is', 'no', 'universal', 'recipe', 'for', 'how', 'to', 'choose', 'the', 'right', 'the', 'only', 'true', 'only', 'you', 'destined', 'path', 'in', 'life']\n",
            "[84, 104, 101, 114, 101, 105, 115, 110, 111, 117, 110, 105, 118, 101, 114, 115, 97, 108, 114, 101, 99, 105, 112, 101, 102, 111, 114, 104, 111, 119, 116, 111, 99, 104, 111, 111, 115, 101, 116, 104, 101, 114, 105, 103, 104, 116, 44, 116, 104, 101, 111, 110, 108, 121, 116, 114, 117, 101, 44, 111, 110, 108, 121, 121, 111, 117, 100, 101, 115, 116, 105, 110, 101, 100, 112, 97, 116, 104, 105, 110, 108, 105, 102, 101, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A67F8N3t20xx"
      }
    }
  ]
}